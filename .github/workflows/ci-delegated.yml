name: Delegated Validation CI

on:
  schedule:
    # Run daily at 11 AM UTC - Delegated validation using Prysm
    - cron: '0 11 * * *'
  workflow_dispatch:
    inputs:
      duration:
        description: 'Test duration (e.g., 5m, 10m, 30m)'
        default: '30m'
        required: false
      chain:
        description: 'Chain'
        default: 'mainnet'
        required: false
      region:
        description: 'Prysm region'
        type: choice
        options:
          - SFO
          - SYD
        default: 'SFO'
        required: false
      clear_history:
        description: 'Clear all historical reports and start fresh'
        type: boolean
        default: false
        required: false

jobs:
  delegated-validation-test:
    runs-on: ubuntu-latest
    timeout-minutes: 120

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.21'

      - name: Install Dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y jq bc

      - name: Configure Go Module for Delegated Validation
        run: |
          echo "ðŸ”— Configuring Go module for delegated validation mode"
          
          # Update go.mod for delegated validation using the built-in function
          go run . --validation-mode=delegated --update-go-mod
          
          # Ensure dependencies are properly resolved
          go mod tidy
          
          # Show the go.mod changes
          echo "Go module configuration:"
          cat go.mod | grep -A5 -B5 "hermes\|replace" || echo "No replace directives found"

      - name: Build Peer Score Tool
        run: |
          echo "ðŸ”§ Building peer score tool for delegated validation"
          go build -o peer-score-tool

      - name: Validate Go Module Configuration
        run: |
          echo "âœ… Validating Go module configuration for delegated validation"
          go run . --validation-mode=delegated --validate-go-mod

      - name: Run Delegated Validation Test
        env:
          DURATION: ${{ github.event.inputs.duration || '30m' }}
          CHAIN: ${{ github.event.inputs.chain || 'mainnet' }}
          REGION: ${{ github.event.inputs.region || 'SFO' }}
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
        run: |
          # Select Prysm host based on region
          if [ "$REGION" = "SYD" ]; then
            PRYSM_HOST="${{ secrets.PRYSM_HOST_SYD }}"
          else
            PRYSM_HOST="${{ secrets.PRYSM_HOST_SFO }}"
          fi

          echo "ðŸ”— Running delegated validation test using Prysm host in $REGION region"

          ./peer-score-tool \
            --validation-mode=delegated \
            --prysm-host="$PRYSM_HOST" \
            --prysm-http-port=443 \
            --prysm-grpc-port=443 \
            --duration="$DURATION" \
            --timestamp-files \
            --output=peer-score-report-delegated.json

      - name: Generate HTML Report
        run: |
          if [ -f peer-score-report-delegated.json ]; then
            echo "ðŸ“Š Generating HTML report for delegated validation"
            
            # Create timestamp-based directory structure
            TIMESTAMP=$(date -u +"%Y-%m-%d_%H-%M-%S")
            DATE_DIR=$(date -u +"%Y-%m-%d")
            MODE_DIR="delegated"

            # Ensure the reports directory structure exists
            mkdir -p reports/$DATE_DIR/$MODE_DIR

            # Check if this is a same-day re-run and preserve existing reports
            if [ -d "reports/$DATE_DIR/$MODE_DIR" ] && [ "$(ls -A reports/$DATE_DIR/$MODE_DIR 2>/dev/null)" ]; then
              echo "Found existing delegated reports for $DATE_DIR, they will be preserved"
              ls -la "reports/$DATE_DIR/$MODE_DIR/"
            fi

            # Copy JSON report with timestamp and mode
            cp peer-score-report-delegated.json reports/$DATE_DIR/$MODE_DIR/peer-score-report-delegated-$TIMESTAMP.json

            # Copy HTML report (should be generated automatically by tool)
            if [ -f peer-score-report-delegated.html ]; then
              # First fix the data file reference in the HTML to point to the timestamped version
              sed "s/peer-score-report-delegated-data.js/peer-score-report-delegated-$TIMESTAMP-data.js/g" peer-score-report-delegated.html > reports/$DATE_DIR/$MODE_DIR/peer-score-report-delegated-$TIMESTAMP.html

              # Copy the JavaScript data file that's generated with the HTML report
              if [ -f peer-score-report-delegated-data.js ]; then
                cp peer-score-report-delegated-data.js reports/$DATE_DIR/$MODE_DIR/peer-score-report-delegated-$TIMESTAMP-data.js
              else
                echo "Warning: JavaScript data file not found, HTML report may not function properly"
              fi

            else
              echo "HTML report not found, tool may have failed to generate it"
              exit 1
            fi
          else
            echo "No JSON report found"
            exit 1
          fi

      - name: Generate Historical Index
        env:
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
          CLEAR_HISTORY: ${{ github.event.inputs.clear_history || 'false' }}
          VALIDATION_MODE: 'delegated'
        run: |
          if [ "$CLEAR_HISTORY" = "true" ]; then
            echo "ðŸ§¹ Clear history requested - starting fresh without downloading historical delegated reports"
            echo "Skipping historical report preservation..."
          else
            # Download existing reports manifest from GitHub Pages to preserve history
            echo "Attempting to preserve existing delegated reports from GitHub Pages using manifest..."

            # Try to download the existing reports manifest
            MANIFEST_URL="https://${{ github.repository_owner }}.github.io/hermes-peer-score/reports-manifest.json"
            echo "Downloading reports manifest from: $MANIFEST_URL"

            if curl -f -s "$MANIFEST_URL" -o reports-manifest.json 2>/dev/null; then
              echo "Found existing reports manifest, downloading historical delegated reports..."

              # Set cutoff date for 28-day retention
              CUTOFF_DATE=$(date -u -d "28 days ago" +"%Y-%m-%d")
              echo "Cutoff date for retention: $CUTOFF_DATE"

              # Download historical reports using the manifest
              CUTOFF_DATE="$CUTOFF_DATE" VALIDATION_MODE="delegated" python3 scripts/download_reports.py
            else
              echo "No existing reports manifest found - this might be the first delegated run"
            fi
          fi

          if [ "$CLEAR_HISTORY" = "true" ]; then
            echo "ðŸ§¹ Clearing all historical delegated reports as requested..."
            # Remove all delegated report directories except the current one
            find reports -maxdepth 2 -type d -path "*/delegated" | while read dir; do
              DIR_DATE=$(basename $(dirname "$dir"))
              CURRENT_DATE=$(date -u +"%Y-%m-%d")
              if [ "$DIR_DATE" != "$CURRENT_DATE" ]; then
                echo "Removing historical delegated reports from $DIR_DATE"
                rm -rf "$dir"
              fi
            done
          else
            # Clean up any delegated directories older than 28 days
            echo "Cleaning up delegated reports older than 28 days..."
            find reports -maxdepth 2 -type d -path "*/delegated" | while read dir; do
              DIR_DATE=$(basename $(dirname "$dir"))
              if [ "$DIR_DATE" != "reports" ]; then
                # Calculate if this directory is older than 28 days
                DIR_EPOCH=$(date -d "$DIR_DATE" +%s 2>/dev/null || echo "0")
                CUTOFF_EPOCH=$(date -d "28 days ago" +%s)

                if [ "$DIR_EPOCH" -lt "$CUTOFF_EPOCH" ] && [ "$DIR_EPOCH" -gt "0" ]; then
                  echo "Removing old delegated reports from $DIR_DATE"
                  rm -rf "$dir"
                fi
              fi
            done
          fi

          echo "Delegated cleanup complete, generating new index..."

          # Generate the historical index page and manifest
          VALIDATION_MODE="delegated" python3 scripts/generate_index.py

      - name: Upload Report Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: delegated-validation-reports-${{ github.run_number }}
          path: reports/
          retention-days: 30

      - name: Setup Pages
        uses: actions/configure-pages@v5

      - name: Upload to Pages
        uses: actions/upload-pages-artifact@v3
        with:
          path: reports/

      - name: Parse Report Results
        id: results
        run: |
          if [ -f peer-score-report-delegated.json ]; then
            # Extract data from the new JSON format
            validation_mode=$(jq -r '.validation_mode // "delegated"' peer-score-report-delegated.json)
            hermes_version=$(jq -r '.validation_config.hermes_version // "unknown"' peer-score-report-delegated.json)
            total_connections=$(jq -r '.total_connections // 0' peer-score-report-delegated.json)
            successful_handshakes=$(jq -r '.successful_handshakes // 0' peer-score-report-delegated.json)
            failed_handshakes=$(jq -r '.failed_handshakes // 0' peer-score-report-delegated.json)
            unique_peers=$(jq -r '.peers | length' peer-score-report-delegated.json)
            duration_seconds=$(jq -r '.duration / 1000000000' peer-score-report-delegated.json)
            test_duration=$(jq -r '.config.TestDuration / 1000000000' peer-score-report-delegated.json)
            
            # Extract validation metrics
            validation_success_rate=$(jq -r '.validation_metrics.validation_success_rate // 0' peer-score-report-delegated.json)
            avg_latency_ms=$(jq -r '(.validation_metrics.validation_latency.average // 0) / 1000000' peer-score-report-delegated.json)
            messages_per_sec=$(jq -r '.validation_metrics.message_processing_rate.messages_per_second // 0' peer-score-report-delegated.json)
            error_rate=$(jq -r '.validation_metrics.error_rates.error_rate // 0' peer-score-report-delegated.json)

            # Count unique client types
            unique_clients=$(jq -r '[.peers[].client_type] | unique | length' peer-score-report-delegated.json)

            # Calculate success rate
            if [ "$total_connections" -gt 0 ]; then
              success_rate=$(echo "scale=1; $successful_handshakes * 100 / $total_connections" | bc)
            else
              success_rate="0"
            fi

            # Count total events
            total_events=$(jq -r '[.peer_event_counts[] | add] | add // 0' peer-score-report-delegated.json)

            # Set outputs
            echo "validation_mode=$validation_mode" >> $GITHUB_OUTPUT
            echo "hermes_version=$hermes_version" >> $GITHUB_OUTPUT
            echo "connections=$total_connections" >> $GITHUB_OUTPUT
            echo "successful_handshakes=$successful_handshakes" >> $GITHUB_OUTPUT
            echo "failed_handshakes=$failed_handshakes" >> $GITHUB_OUTPUT
            echo "success_rate=$success_rate" >> $GITHUB_OUTPUT
            echo "unique_peers=$unique_peers" >> $GITHUB_OUTPUT
            echo "unique_clients=$unique_clients" >> $GITHUB_OUTPUT
            echo "duration_seconds=$duration_seconds" >> $GITHUB_OUTPUT
            echo "test_duration=$test_duration" >> $GITHUB_OUTPUT
            echo "total_events=$total_events" >> $GITHUB_OUTPUT
            echo "validation_success_rate=$validation_success_rate" >> $GITHUB_OUTPUT
            echo "avg_latency_ms=$avg_latency_ms" >> $GITHUB_OUTPUT
            echo "messages_per_sec=$messages_per_sec" >> $GITHUB_OUTPUT
            echo "error_rate=$error_rate" >> $GITHUB_OUTPUT
          else
            echo "No JSON report found"
            exit 1
          fi

      - name: Generate Summary
        run: |
          # Use the parsed results
          validation_mode=${{ steps.results.outputs.validation_mode }}
          hermes_version=${{ steps.results.outputs.hermes_version }}
          connections=${{ steps.results.outputs.connections }}
          successful_handshakes=${{ steps.results.outputs.successful_handshakes }}
          failed_handshakes=${{ steps.results.outputs.failed_handshakes }}
          success_rate=${{ steps.results.outputs.success_rate }}
          unique_peers=${{ steps.results.outputs.unique_peers }}
          unique_clients=${{ steps.results.outputs.unique_clients }}
          duration_seconds=${{ steps.results.outputs.duration_seconds }}
          test_duration=${{ steps.results.outputs.test_duration }}
          total_events=${{ steps.results.outputs.total_events }}
          validation_success_rate=${{ steps.results.outputs.validation_success_rate }}
          avg_latency_ms=${{ steps.results.outputs.avg_latency_ms }}
          messages_per_sec=${{ steps.results.outputs.messages_per_sec }}
          error_rate=${{ steps.results.outputs.error_rate }}
          clear_history=${{ github.event.inputs.clear_history || 'false' }}

          {
            echo "## ðŸ”— Delegated Validation Report"
            echo ""
            if [ "$clear_history" = "true" ]; then
              echo "### ðŸ§¹ History Cleared"
              echo "- **Historical reports were cleared** - Starting fresh with this report"
              echo ""
            fi
            echo "### Validation Configuration"
            echo "- **Mode:** Delegated Validation (via Prysm)"
            echo "- **Hermes Version:** $hermes_version"
            echo "- **Test Duration:** ${test_duration}s"
            echo "- **Actual Duration:** ${duration_seconds}s"
            echo ""
            echo "### Connection Statistics"
            echo "- **Total Connections:** $connections"
            echo "- **Successful Handshakes:** $successful_handshakes"
            echo "- **Failed Handshakes:** $failed_handshakes"
            echo "- **Success Rate:** ${success_rate}%"
            echo ""
            echo "### Validation Performance"
            echo "- **Validation Success Rate:** $(echo "scale=1; $validation_success_rate * 100" | bc)%"
            echo "- **Average Latency:** ${avg_latency_ms}ms"
            echo "- **Message Processing Rate:** ${messages_per_sec} msg/sec"
            echo "- **Error Rate:** $(echo "scale=2; $error_rate * 100" | bc)%"
            echo ""
            echo "### Network Diversity"
            echo "- **Unique Peers Discovered:** $unique_peers"
            echo "- **Unique Client Types:** $unique_clients"
            echo "- **Total Events Captured:** $total_events"
            echo ""
            echo "### Report Files"
            echo "- ðŸ“Š [Interactive HTML Report](https://ethpandaops.github.io/hermes-peer-score/)"
            echo "- ðŸ“„ [Raw JSON Data](https://ethpandaops.github.io/hermes-peer-score/)"
            echo ""
            echo "_Delegated validation report generated on $(date) using Hermes $hermes_version_"
          } >> $GITHUB_STEP_SUMMARY

  deploy:
    needs: delegated-validation-test
    runs-on: ubuntu-latest

    permissions:
      pages: write
      id-token: write

    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}

    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4